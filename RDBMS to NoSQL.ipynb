{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff3fdd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "\n",
    "def aggregate_and_convert_to_nosql(csv_files, json_folder):\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(json_folder, exist_ok=True)\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        # Extract the table name from the CSV file name\n",
    "        table_name = os.path.splitext(os.path.basename(csv_file))[0]\n",
    "\n",
    "        # Open CSV file for reading\n",
    "        with open(csv_file, 'r', encoding='utf-8') as csv_file:\n",
    "            # Create a CSV reader\n",
    "            csv_reader = csv.DictReader(csv_file)\n",
    "\n",
    "            # Initialize an empty dictionary for aggregated data\n",
    "            aggregated_data = {}\n",
    "\n",
    "            # Iterate through each row in the CSV file\n",
    "            for row in csv_reader:\n",
    "                # Assuming 'id' is the unique identifier\n",
    "                unique_id = row['id']\n",
    "\n",
    "                # Check if the unique_id is not in the aggregated_data dictionary\n",
    "                if unique_id not in aggregated_data:\n",
    "                    aggregated_data[unique_id] = {}\n",
    "\n",
    "                # Replace placeholder values with actual column names\n",
    "                username_column = row.get('user_name_column', 'username')\n",
    "                email_column = row.get('email_address_column', 'email')\n",
    "\n",
    "                # Aggregate data into the NoSQL schema\n",
    "                aggregated_data[unique_id][table_name] = {\n",
    "                    'id': unique_id,\n",
    "                    \"username\": row.get(username_column, ''),\n",
    "                    \"email\": row.get(email_column, ''),\n",
    "                    \"reputation\": row.get('reputation_column', 0),\n",
    "                    \"creation_date\": row.get('creation_date_column', ''),\n",
    "                    \"last_access_date\": row.get('last_access_date_column', ''),\n",
    "                    \"location\": row.get('location_column', ''),\n",
    "                    \"badges\": {\n",
    "                        \"gold\": row.get('gold_badges_column', 0),\n",
    "                        \"silver\": row.get('silver_badges_column', 0),\n",
    "                        \"bronze\": row.get('bronze_badges_column', 0),\n",
    "                    },\n",
    "                    \"posts\": [\n",
    "                        {\n",
    "                            \"post_id\": row.get('post_id_column_1', ''),\n",
    "                            \"title\": row.get('post_title_column_1', ''),\n",
    "                            \"creation_date\": row.get('post_creation_date_column_1', ''),\n",
    "                        },\n",
    "                        {\n",
    "                            \"post_id\": row.get('post_id_column_2', ''),\n",
    "                            \"title\": row.get('post_title_column_2', ''),\n",
    "                            \"creation_date\": row.get('post_creation_date_column_2', ''),\n",
    "                        },\n",
    "                        # Add more post fields as needed\n",
    "                    ],\n",
    "                    \"comments\": [\n",
    "                        {\n",
    "                            \"comment_id\": row.get('comment_id_column_1', ''),\n",
    "                            \"text\": row.get('comment_text_column_1', ''),\n",
    "                            \"creation_date\": row.get('comment_creation_date_column_1', ''),\n",
    "                        },\n",
    "                        {\n",
    "                            \"comment_id\": row.get('comment_id_column_2', ''),\n",
    "                            \"text\": row.get('comment_text_column_2', ''),\n",
    "                            \"creation_date\": row.get('comment_creation_date_column_2', ''),\n",
    "                        },\n",
    "                        # Add more comment fields as needed\n",
    "                    ],\n",
    "                    # Add more fields as needed\n",
    "                }\n",
    "\n",
    "        # Construct the JSON file path\n",
    "        json_file_path = os.path.join(json_folder, f'{table_name}.json')\n",
    "\n",
    "        # Open JSON file for writing\n",
    "        with open(json_file_path, 'w', encoding='utf-8') as json_file:\n",
    "            # Write the aggregated data to the JSON file\n",
    "            json.dump(list(aggregated_data.values()), json_file, indent=2)\n",
    "\n",
    "# Specify the list of CSV files and the folder for JSON output files\n",
    "csv_files = ['badges.csv', 'post_history.csv', 'post_links.csv',\n",
    "             'posts_answers.csv', 'posts_moderator_nomination.csv',\n",
    "             'posts_orphaned_tag_wiki.csv', 'posts_privilege_wiki.csv',\n",
    "             'posts_questions.csv', 'posts_tag_wiki.csv', 'stackoverflow_posts.csv',\n",
    "             'tags.csv', 'users.csv', 'votes.csv']\n",
    "json_folder = 'path/to/json_output'\n",
    "\n",
    "# Aggregate and convert each CSV file to JSON with NoSQL schema\n",
    "aggregate_and_convert_to_nosql(csv_files, json_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a89380e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "\n",
    "def aggregate_and_convert_to_nosql(csv_files, json_folder):\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(json_folder, exist_ok=True)\n",
    "\n",
    "    # Initialize an empty dictionary for aggregated data\n",
    "    aggregated_data = {}\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        # Extract the table name from the CSV file name\n",
    "        table_name = os.path.splitext(os.path.basename(csv_file))[0]\n",
    "\n",
    "        # Open CSV file for reading\n",
    "        with open(csv_file, 'r', encoding='utf-8') as csv_file:\n",
    "            # Create a CSV reader\n",
    "            csv_reader = csv.DictReader(csv_file)\n",
    "\n",
    "            # Iterate through each row in the CSV file\n",
    "            for row in csv_reader:\n",
    "                # Assuming 'id' is the unique identifier\n",
    "                unique_id = row['id']\n",
    "\n",
    "                # Check if the unique_id is not in the aggregated_data dictionary\n",
    "                if unique_id not in aggregated_data:\n",
    "                    aggregated_data[unique_id] = {}\n",
    "\n",
    "                # Aggregate data into the NoSQL schema\n",
    "                aggregated_data[unique_id][table_name] = {\n",
    "                    'id': unique_id,\n",
    "                    \"username\": row.get('displayname', ''),\n",
    "                    \"email\": row.get('email_hash', ''),\n",
    "                    \"reputation\": row.get('reputation', 0),\n",
    "                    \"creation_date\": row.get('creation_date', ''),\n",
    "                    \"last_access_date\": row.get('last_access_date', ''),\n",
    "                    \"location\": row.get('location', ''),\n",
    "                    \"badges\": {\n",
    "                        \"gold\": row.get('gold_badges', 0),\n",
    "                        \"silver\": row.get('silver_badges', 0),\n",
    "                        \"bronze\": row.get('bronze_badges', 0),\n",
    "                    },\n",
    "                    \"posts\": [\n",
    "                        {\n",
    "                            \"post_id\": row.get('post_id', ''),\n",
    "                            \"title\": row.get('title', ''),\n",
    "                            \"creation_date\": row.get('creation_date_post', ''),\n",
    "                        },\n",
    "                        # Add more post fields as needed\n",
    "                    ],\n",
    "                    \"comments\": [\n",
    "                        {\n",
    "                            \"comment_id\": row.get('comment_id', ''),\n",
    "                            \"text\": row.get('text', ''),\n",
    "                            \"creation_date\": row.get('creation_date_comment', ''),\n",
    "                        },\n",
    "                        # Add more comment fields as needed\n",
    "                    ],\n",
    "                    # Add more fields as needed\n",
    "                    \"additional_field_1\": row.get('additional_field_1', ''),\n",
    "                    \"additional_field_2\": row.get('additional_field_2', ''),\n",
    "                    # Add more additional fields as needed\n",
    "                }\n",
    "\n",
    "    # Construct the JSON file path\n",
    "    json_file_path = os.path.join(json_folder, 'aggregated_data.json')\n",
    "\n",
    "    # Open JSON file for writing\n",
    "    with open(json_file_path, 'w', encoding='utf-8') as json_file:\n",
    "        # Write the aggregated data to the JSON file\n",
    "        json.dump(list(aggregated_data.values()), json_file, indent=2)\n",
    "\n",
    "# Specify the list of CSV files and the folder for JSON output files\n",
    "csv_files = ['badges.csv', 'post_history.csv', 'post_links.csv',\n",
    "             'posts_answers.csv', 'posts_moderator_nomination.csv',\n",
    "             'posts_orphaned_tag_wiki.csv', 'posts_privilege_wiki.csv',\n",
    "             'posts_questions.csv', 'posts_tag_wiki.csv', 'stackoverflow_posts.csv',\n",
    "             'tags.csv', 'users.csv', 'votes.csv']\n",
    "json_folder = 'path/to/json_output'\n",
    "\n",
    "# Aggregate and convert each CSV file to JSON with NoSQL schema\n",
    "aggregate_and_convert_to_nosql(csv_files, json_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68d0135b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table: badges, Rows processed: 500\n",
      "Table: post_history, Rows processed: 500\n",
      "Table: post_links, Rows processed: 500\n",
      "Table: posts_answers, Rows processed: 500\n",
      "Table: posts_moderator_nomination, Rows processed: 342\n",
      "Table: posts_orphaned_tag_wiki, Rows processed: 167\n",
      "Table: posts_privilege_wiki, Rows processed: 2\n",
      "Table: posts_questions, Rows processed: 500\n",
      "Table: posts_tag_wiki, Rows processed: 500\n",
      "Table: stackoverflow_posts, Rows processed: 500\n",
      "Table: tags, Rows processed: 500\n",
      "Table: users, Rows processed: 500\n",
      "Table: votes, Rows processed: 500\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "\n",
    "def aggregate_and_convert_to_nosql(csv_files, json_folder):\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(json_folder, exist_ok=True)\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        # Extract the table name from the CSV file name\n",
    "        table_name = os.path.splitext(os.path.basename(csv_file))[0]\n",
    "\n",
    "        # Open CSV file for reading\n",
    "        with open(csv_file, 'r', encoding='utf-8') as csv_file:\n",
    "            # Create a CSV reader\n",
    "            csv_reader = csv.DictReader(csv_file)\n",
    "\n",
    "            # Initialize an empty dictionary for aggregated data\n",
    "            aggregated_data = {}\n",
    "\n",
    "            # Iterate through each row in the CSV file\n",
    "            for row in csv_reader:\n",
    "                # Assuming 'id' is the unique identifier\n",
    "                unique_id = row['id']\n",
    "\n",
    "                # Check if the unique_id is not in the aggregated_data dictionary\n",
    "                if unique_id not in aggregated_data:\n",
    "                    aggregated_data[unique_id] = {}\n",
    "\n",
    "                # Replace placeholder values with actual column names\n",
    "                username_column = row.get('user_name_column', 'display_name')\n",
    "                email_column = row.get('email_address_column', 'email')\n",
    "                reputation_column = row.get('reputation_column', 'reputation')\n",
    "                creation_date_column = row.get('creation_date_column', 'creation_date')\n",
    "                last_access_date_column = row.get('last_access_date_column', 'last_access_date')\n",
    "                location_column = row.get('location_column', 'location')\n",
    "                gold_badges_column = row.get('gold_badges_column', 'gold_badges')\n",
    "                silver_badges_column = row.get('silver_badges_column', 'silver_badges')\n",
    "                bronze_badges_column = row.get('bronze_badges_column', 'bronze_badges')\n",
    "                post_id_column_1 = row.get('post_id_column_1', 'post_id_1')\n",
    "                post_title_column_1 = row.get('post_title_column_1', 'post_title_1')\n",
    "                post_creation_date_column_1 = row.get('post_creation_date_column_1', 'post_creation_date_1')\n",
    "                post_id_column_2 = row.get('post_id_column_2', 'post_id_2')\n",
    "                post_title_column_2 = row.get('post_title_column_2', 'post_title_2')\n",
    "                post_creation_date_column_2 = row.get('post_creation_date_column_2', 'post_creation_date_2')\n",
    "                comment_id_column_1 = row.get('comment_id_column_1', 'comment_id_1')\n",
    "                comment_text_column_1 = row.get('comment_text_column_1', 'comment_text_1')\n",
    "                comment_creation_date_column_1 = row.get('comment_creation_date_column_1', 'comment_creation_date_1')\n",
    "                comment_id_column_2 = row.get('comment_id_column_2', 'comment_id_2')\n",
    "                comment_text_column_2 = row.get('comment_text_column_2', 'comment_text_2')\n",
    "                comment_creation_date_column_2 = row.get('comment_creation_date_column_2', 'comment_creation_date_2')\n",
    "\n",
    "                # Aggregate data into the NoSQL schema\n",
    "                aggregated_data[unique_id][table_name] = {\n",
    "                    'id': unique_id,\n",
    "                    \"username\": row.get(username_column, ''),\n",
    "                    \"email\": row.get(email_column, ''),\n",
    "                    \"reputation\": row.get(reputation_column, 0),\n",
    "                    \"creation_date\": row.get(creation_date_column, ''),\n",
    "                    \"last_access_date\": row.get(last_access_date_column, ''),\n",
    "                    \"location\": row.get(location_column, ''),\n",
    "                    \"badges\": {\n",
    "                        \"gold\": row.get(gold_badges_column, 0),\n",
    "                        \"silver\": row.get(silver_badges_column, 0),\n",
    "                        \"bronze\": row.get(bronze_badges_column, 0),\n",
    "                    },\n",
    "                    \"posts\": [\n",
    "                        {\n",
    "                            \"post_id\": row.get(post_id_column_1, ''),\n",
    "                            \"title\": row.get(post_title_column_1, ''),\n",
    "                            \"creation_date\": row.get(post_creation_date_column_1, ''),\n",
    "                        },\n",
    "                        {\n",
    "                            \"post_id\": row.get(post_id_column_2, ''),\n",
    "                            \"title\": row.get(post_title_column_2, ''),\n",
    "                            \"creation_date\": row.get(post_creation_date_column_2, ''),\n",
    "                        },\n",
    "                        # Add more post fields as needed\n",
    "                    ],\n",
    "                    \"comments\": [\n",
    "                        {\n",
    "                            \"comment_id\": row.get(comment_id_column_1, ''),\n",
    "                            \"text\": row.get(comment_text_column_1, ''),\n",
    "                            \"creation_date\": row.get(comment_creation_date_column_1, ''),\n",
    "                        },\n",
    "                        {\n",
    "                            \"comment_id\": row.get(comment_id_column_2, ''),\n",
    "                            \"text\": row.get(comment_text_column_2, ''),\n",
    "                            \"creation_date\": row.get(comment_creation_date_column_2, ''),\n",
    "                        },\n",
    "                        # Add more comment fields as needed\n",
    "                    ],\n",
    "                    # Add more fields as needed\n",
    "                }\n",
    "\n",
    "            # Log the number of rows processed for each table\n",
    "            print(f\"Table: {table_name}, Rows processed: {len(aggregated_data)}\")\n",
    "\n",
    "        # Construct the JSON file path\n",
    "        json_file_path = os.path.join(json_folder, f'{table_name}.json')\n",
    "\n",
    "        # Open JSON file for writing\n",
    "        with open(json_file_path, 'w', encoding='utf-8') as json_file:\n",
    "            # Write the aggregated data to the JSON file\n",
    "            json.dump(list(aggregated_data.values()), json_file, indent=2)\n",
    "\n",
    "# Specify the list of CSV files and the folder for JSON output files\n",
    "csv_files = ['badges.csv', 'post_history.csv', 'post_links.csv',\n",
    "             'posts_answers.csv', 'posts_moderator_nomination.csv',\n",
    "             'posts_orphaned_tag_wiki.csv', 'posts_privilege_wiki.csv',\n",
    "             'posts_questions.csv', 'posts_tag_wiki.csv', 'stackoverflow_posts.csv',\n",
    "             'tags.csv', 'users.csv', 'votes.csv']\n",
    "json_folder = 'path/to/json_output'\n",
    "\n",
    "# Aggregate and convert each CSV file to JSON with NoSQL schema\n",
    "aggregate_and_convert_to_nosql(csv_files, json_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace87b9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cfc6604c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "\n",
    "def aggregate_and_convert_to_nosql(csv_files, json_folder):\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(json_folder, exist_ok=True)\n",
    "\n",
    "    # Initialize dictionaries for aggregated data\n",
    "    users_data = {}\n",
    "    badges_data = {}\n",
    "    questions_data = {}\n",
    "    other_data = {}\n",
    "\n",
    "    # Iterate through each CSV file\n",
    "    for csv_file in csv_files:\n",
    "        # Extract the table name from the CSV file name\n",
    "        table_name = os.path.splitext(os.path.basename(csv_file))[0]\n",
    "\n",
    "        # Open CSV file for reading\n",
    "        with open(csv_file, 'r', encoding='utf-8') as csv_file:\n",
    "            # Create a CSV reader\n",
    "            csv_reader = csv.DictReader(csv_file)\n",
    "\n",
    "            # Iterate through each row in the CSV file\n",
    "            for row in csv_reader:\n",
    "                # Use a different column as the unique identifier for tables without 'id'\n",
    "                unique_id = row.get('user_id', row.get('post_id', None))\n",
    "\n",
    "                if unique_id is not None:\n",
    "                    # Check if the unique_id is not in the aggregated_data dictionary\n",
    "                    if unique_id not in users_data:\n",
    "                        users_data[unique_id] = {'id': unique_id}\n",
    "                    else:\n",
    "                        users_data[unique_id].update(row)\n",
    "\n",
    "                    # Example: Add other tables to specific dictionaries\n",
    "                    if table_name == 'badges':\n",
    "                        if unique_id not in badges_data:\n",
    "                            badges_data[unique_id] = {'id': unique_id}\n",
    "                        badges_data[unique_id].update(row)\n",
    "                    elif table_name == 'posts_questions':\n",
    "                        if unique_id not in questions_data:\n",
    "                            questions_data[unique_id] = {'id': unique_id}\n",
    "                        questions_data[unique_id].update(row)\n",
    "                    else:\n",
    "                        if unique_id not in other_data:\n",
    "                            other_data[unique_id] = {'id': unique_id}\n",
    "                        other_data[unique_id].update({table_name: row})\n",
    "\n",
    "    # Construct the JSON file paths\n",
    "    users_json_path = os.path.join(json_folder, 'users.json')\n",
    "    badges_json_path = os.path.join(json_folder, 'badges.json')\n",
    "    questions_json_path = os.path.join(json_folder, 'questions.json')\n",
    "    other_data_json_path = os.path.join(json_folder, 'other_data.json')\n",
    "\n",
    "    # Write data to JSON files\n",
    "    with open(users_json_path, 'w', encoding='utf-8') as users_json_file:\n",
    "        json.dump(list(users_data.values()), users_json_file, indent=2)\n",
    "\n",
    "    with open(badges_json_path, 'w', encoding='utf-8') as badges_json_file:\n",
    "        json.dump(list(badges_data.values()), badges_json_file, indent=2)\n",
    "\n",
    "    with open(questions_json_path, 'w', encoding='utf-8') as questions_json_file:\n",
    "        json.dump(list(questions_data.values()), questions_json_file, indent=2)\n",
    "\n",
    "    with open(other_data_json_path, 'w', encoding='utf-8') as other_data_json_file:\n",
    "        json.dump(list(other_data.values()), other_data_json_file, indent=2)\n",
    "\n",
    "# Specify the list of CSV files and the folder for JSON output files\n",
    "csv_files = ['badges.csv', 'post_history.csv', 'post_links.csv',\n",
    "             'posts_answers.csv', 'posts_moderator_nomination.csv',\n",
    "             'posts_orphaned_tag_wiki.csv', 'posts_privilege_wiki.csv',\n",
    "             'posts_questions.csv', 'posts_tag_wiki.csv', 'stackoverflow_posts.csv',\n",
    "             'tags.csv', 'users.csv', 'votes.csv']\n",
    "json_folder = 'path/to/json_output'\n",
    "\n",
    "# Aggregate and convert each CSV file to JSON with NoSQL schema\n",
    "aggregate_and_convert_to_nosql(csv_files, json_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fab1bc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "\n",
    "def aggregate_and_convert_to_nosql(csv_files, json_folder):\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(json_folder, exist_ok=True)\n",
    "\n",
    "    # Initialize dictionaries for aggregated data\n",
    "    users_data = {}\n",
    "    badges_data = {}\n",
    "    questions_data = {}\n",
    "\n",
    "    # Iterate through each CSV file\n",
    "    for csv_file in csv_files:\n",
    "        # Extract the table name from the CSV file name\n",
    "        table_name = os.path.splitext(os.path.basename(csv_file))[0]\n",
    "\n",
    "        # Open CSV file for reading\n",
    "        with open(csv_file, 'r', encoding='utf-8') as csv_file:\n",
    "            # Create a CSV reader\n",
    "            csv_reader = csv.DictReader(csv_file)\n",
    "\n",
    "            # Iterate through each row in the CSV file\n",
    "            for row in csv_reader:\n",
    "                # Use a different column as the unique identifier for tables without 'id'\n",
    "                unique_id = row.get('user_id', row.get('post_id', None))\n",
    "\n",
    "                if unique_id is not None:\n",
    "                    # Check if the unique_id is not in the aggregated_data dictionary\n",
    "                    if unique_id not in users_data:\n",
    "                        users_data[unique_id] = {'id': unique_id}\n",
    "\n",
    "                    # Update the 'users' dictionary with information from other tables\n",
    "                    users_data[unique_id].update(row)\n",
    "\n",
    "                    # Example: Add other tables to specific dictionaries\n",
    "                    if table_name == 'badges':\n",
    "                        if unique_id not in badges_data:\n",
    "                            badges_data[unique_id] = {'id': unique_id}\n",
    "                        badges_data[unique_id].update(row)\n",
    "                    elif table_name == 'posts_questions':\n",
    "                        if unique_id not in questions_data:\n",
    "                            questions_data[unique_id] = {'id': unique_id}\n",
    "                        questions_data[unique_id].update(row)\n",
    "\n",
    "    # Construct the JSON file paths\n",
    "    users_json_path = os.path.join(json_folder, 'users.json')\n",
    "    badges_json_path = os.path.join(json_folder, 'badges.json')\n",
    "    questions_json_path = os.path.join(json_folder, 'questions.json')\n",
    "\n",
    "    # Write data to JSON files\n",
    "    with open(users_json_path, 'w', encoding='utf-8') as users_json_file:\n",
    "        json.dump(list(users_data.values()), users_json_file, indent=2)\n",
    "\n",
    "    with open(badges_json_path, 'w', encoding='utf-8') as badges_json_file:\n",
    "        json.dump(list(badges_data.values()), badges_json_file, indent=2)\n",
    "\n",
    "    with open(questions_json_path, 'w', encoding='utf-8') as questions_json_file:\n",
    "        json.dump(list(questions_data.values()), questions_json_file, indent=2)\n",
    "\n",
    "# Specify the list of CSV files and the folder for JSON output files\n",
    "csv_files = ['badges.csv', 'post_history.csv', 'post_links.csv',\n",
    "             'posts_answers.csv', 'posts_moderator_nomination.csv',\n",
    "             'posts_orphaned_tag_wiki.csv', 'posts_privilege_wiki.csv',\n",
    "             'posts_questions.csv', 'posts_tag_wiki.csv', 'stackoverflow_posts.csv',\n",
    "             'tags.csv', 'users.csv', 'votes.csv']\n",
    "json_folder = 'path/to/json_output'\n",
    "\n",
    "# Aggregate and convert each CSV file to JSON with NoSQL schema\n",
    "aggregate_and_convert_to_nosql(csv_files, json_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c11047e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "\n",
    "def aggregate_and_convert_to_nosql(csv_files, json_folder):\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(json_folder, exist_ok=True)\n",
    "\n",
    "    # Initialize a dictionary to store aggregated user data\n",
    "    users_data = {}\n",
    "\n",
    "    # Iterate through each CSV file\n",
    "    for csv_file in csv_files:\n",
    "        # Extract the table name from the CSV file name\n",
    "        table_name = os.path.splitext(os.path.basename(csv_file))[0]\n",
    "\n",
    "        # Open CSV file for reading\n",
    "        with open(csv_file, 'r', encoding='utf-8') as csv_file:\n",
    "            # Create a CSV reader\n",
    "            csv_reader = csv.DictReader(csv_file)\n",
    "\n",
    "            # Iterate through each row in the CSV file\n",
    "            for row in csv_reader:\n",
    "                # Use 'display_name' as the unique identifier\n",
    "                user_id = row.get('display_name', None)\n",
    "\n",
    "                if user_id is not None:\n",
    "                    # Check if the user_id is not in the users_data dictionary\n",
    "                    if user_id not in users_data:\n",
    "                        users_data[user_id] = {'display_name': user_id}\n",
    "\n",
    "                    # Update the user dictionary with information from other tables\n",
    "                    users_data[user_id].update(row)\n",
    "\n",
    "    # Construct the JSON file path for users\n",
    "    users_json_path = os.path.join(json_folder, 'users.json')\n",
    "\n",
    "    # Write data to the users JSON file\n",
    "    with open(users_json_path, 'w', encoding='utf-8') as users_json_file:\n",
    "        json.dump(list(users_data.values()), users_json_file, indent=2)\n",
    "\n",
    "# Specify the list of CSV files and the folder for JSON output files\n",
    "csv_files = ['badges.csv', 'post_history.csv', 'post_links.csv',\n",
    "             'posts_answers.csv', 'posts_moderator_nomination.csv',\n",
    "             'posts_orphaned_tag_wiki.csv', 'posts_privilege_wiki.csv',\n",
    "             'posts_questions.csv', 'posts_tag_wiki.csv', 'stackoverflow_posts.csv',\n",
    "             'tags.csv', 'users.csv', 'votes.csv']\n",
    "json_folder = 'path/to/json_output'\n",
    "\n",
    "# Aggregate and convert each CSV file to JSON with NoSQL schema\n",
    "aggregate_and_convert_to_nosql(csv_files, json_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fabf77eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "\n",
    "def aggregate_and_convert_to_nosql(csv_files, json_folder):\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(json_folder, exist_ok=True)\n",
    "\n",
    "    # Initialize a dictionary to store aggregated user data\n",
    "    users_data = {}\n",
    "\n",
    "    # Iterate through each CSV file\n",
    "    for csv_file in csv_files:\n",
    "        # Extract the table name from the CSV file name\n",
    "        table_name = os.path.splitext(os.path.basename(csv_file))[0]\n",
    "\n",
    "        # Open CSV file for reading\n",
    "        with open(csv_file, 'r', encoding='utf-8') as csv_file:\n",
    "            # Create a CSV reader\n",
    "            csv_reader = csv.DictReader(csv_file)\n",
    "\n",
    "            # Iterate through each row in the CSV file\n",
    "            for row in csv_reader:\n",
    "                # Use 'display_name' as the unique identifier\n",
    "                user_id = row.get('display_name', None)\n",
    "\n",
    "                if user_id is not None:\n",
    "                    # Check if the user_id is not in the users_data dictionary\n",
    "                    if user_id not in users_data:\n",
    "                        users_data[user_id] = {'display_name': user_id}\n",
    "\n",
    "                    # Update the user dictionary with information from other tables\n",
    "                    users_data[user_id].update(row)\n",
    "\n",
    "    # Construct the JSON file path for users\n",
    "    users_json_path = os.path.join(json_folder, 'users.json')\n",
    "\n",
    "    # Write data to the users JSON file\n",
    "    with open(users_json_path, 'w', encoding='utf-8') as users_json_file:\n",
    "        json.dump(list(users_data.values()), users_json_file, indent=2)\n",
    "\n",
    "# Specify the list of CSV files and the folder for JSON output files\n",
    "csv_files = ['badges.csv', 'post_history.csv', 'post_links.csv',\n",
    "             'posts_answers.csv', 'posts_moderator_nomination.csv',\n",
    "             'posts_orphaned_tag_wiki.csv', 'posts_privilege_wiki.csv',\n",
    "             'posts_questions.csv', 'posts_tag_wiki.csv', 'stackoverflow_posts.csv',\n",
    "             'tags.csv', 'users.csv', 'votes.csv']\n",
    "json_folder = 'path/to/json_output'\n",
    "\n",
    "# Aggregate and convert each CSV file to JSON with NoSQL schema\n",
    "aggregate_and_convert_to_nosql(csv_files, json_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7fa51f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add badges.csv info into users.json\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "\n",
    "def aggregate_and_convert_to_nosql(csv_files, json_folder):\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(json_folder, exist_ok=True)\n",
    "\n",
    "    # Initialize dictionaries to store aggregated data\n",
    "    users_data = {}\n",
    "\n",
    "    # Iterate through each CSV file\n",
    "    for csv_file in csv_files:\n",
    "        # Extract the table name from the CSV file name\n",
    "        table_name = os.path.splitext(os.path.basename(csv_file))[0]\n",
    "\n",
    "        # Open CSV file for reading\n",
    "        with open(csv_file, 'r', encoding='utf-8') as csv_file:\n",
    "            # Create a CSV reader\n",
    "            csv_reader = csv.DictReader(csv_file)\n",
    "\n",
    "            # Iterate through each row in the CSV file\n",
    "            for row in csv_reader:\n",
    "                # Assuming 'user_id' is the identifier for users\n",
    "                user_id = row.get('user_id', None)\n",
    "\n",
    "                if user_id is not None:\n",
    "                    # Check if the user_id is not in the users_data dictionary\n",
    "                    if user_id not in users_data:\n",
    "                        users_data[user_id] = {}\n",
    "\n",
    "                    # Update the user dictionary with information from other tables\n",
    "                    users_data[user_id].update(row)\n",
    "\n",
    "                    # For badges, store information in the user dictionary\n",
    "                    if table_name == 'badges':\n",
    "                        if 'badges' not in users_data[user_id]:\n",
    "                            users_data[user_id]['badges'] = []\n",
    "                        users_data[user_id]['badges'].append(row)\n",
    "\n",
    "    # Construct the JSON file path for users\n",
    "    users_json_path = os.path.join(json_folder, 'users.json')\n",
    "\n",
    "    # Write user data to the users.json file\n",
    "    with open(users_json_path, 'w', encoding='utf-8') as users_json_file:\n",
    "        json.dump(list(users_data.values()), users_json_file, indent=2)\n",
    "\n",
    "# Specify the list of CSV files and the folder for JSON output files\n",
    "csv_files = ['badges.csv', 'post_history.csv', 'post_links.csv',\n",
    "             'posts_answers.csv', 'posts_moderator_nomination.csv',\n",
    "             'posts_orphaned_tag_wiki.csv', 'posts_privilege_wiki.csv',\n",
    "             'posts_questions.csv', 'posts_tag_wiki.csv', 'stackoverflow_posts.csv',\n",
    "             'tags.csv', 'users.csv', 'votes.csv']\n",
    "json_folder = 'path/to/json_output'\n",
    "\n",
    "# Aggregate and convert each CSV file to JSON with NoSQL schema\n",
    "aggregate_and_convert_to_nosql(csv_files, json_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "071d92cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "\n",
    "def aggregate_and_convert_to_nosql(csv_files, json_folder):\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(json_folder, exist_ok=True)\n",
    "\n",
    "    # Initialize dictionaries to store aggregated data\n",
    "    users_data = {}\n",
    "    posts_data = {}\n",
    "\n",
    "    # Iterate through each CSV file\n",
    "    for csv_file in csv_files:\n",
    "        # Extract the table name from the CSV file name\n",
    "        table_name = os.path.splitext(os.path.basename(csv_file))[0]\n",
    "\n",
    "        # Open CSV file for reading\n",
    "        with open(csv_file, 'r', encoding='utf-8') as csv_file:\n",
    "            # Create a CSV reader\n",
    "            csv_reader = csv.DictReader(csv_file)\n",
    "\n",
    "            # Iterate through each row in the CSV file\n",
    "            for row in csv_reader:\n",
    "                # Assuming 'user_id' is the identifier for users\n",
    "                user_id = row.get('user_id', None)\n",
    "\n",
    "                # Assuming 'post_id' is the identifier for posts\n",
    "                post_id = row.get('post_id', None)\n",
    "\n",
    "                if user_id is not None:\n",
    "                    # Check if the user_id is not in the users_data dictionary\n",
    "                    if user_id not in users_data:\n",
    "                        users_data[user_id] = {'user_id': user_id, 'posts': []}\n",
    "\n",
    "                    # Update the user dictionary with information from other tables\n",
    "                    users_data[user_id].update(row)\n",
    "\n",
    "                if post_id is not None:\n",
    "                    # Check if the post_id is not in the posts_data dictionary\n",
    "                    if post_id not in posts_data:\n",
    "                        posts_data[post_id] = {'post_id': post_id, 'answers': []}\n",
    "\n",
    "                    # For posts, store information in the post dictionary\n",
    "                    posts_data[post_id].update(row)\n",
    "\n",
    "                    # For answers, add to the list within the post dictionary\n",
    "                    if table_name == 'posts_answers':\n",
    "                        posts_data[post_id]['answers'].append({\n",
    "                            'user_id': row.get('user_id', ''),\n",
    "                            'answer_body': row.get('body', ''),\n",
    "                        })\n",
    "\n",
    "    # Construct the JSON file path for users\n",
    "    users_json_path = os.path.join(json_folder, 'users.json')\n",
    "\n",
    "    # Write user data to the users.json file\n",
    "    with open(users_json_path, 'w', encoding='utf-8') as users_json_file:\n",
    "        json.dump(list(users_data.values()), users_json_file, indent=2)\n",
    "\n",
    "    # Construct the JSON file path for posts\n",
    "    posts_json_path = os.path.join(json_folder, 'posts.json')\n",
    "\n",
    "    # Write post data to the posts.json file\n",
    "    with open(posts_json_path, 'w', encoding='utf-8') as posts_json_file:\n",
    "        json.dump(list(posts_data.values()), posts_json_file, indent=2)\n",
    "\n",
    "# Specify the list of CSV files and the folder for JSON output files\n",
    "csv_files = ['badges.csv', 'post_history.csv', 'post_links.csv',\n",
    "             'posts_answers.csv', 'posts_moderator_nomination.csv',\n",
    "             'posts_orphaned_tag_wiki.csv', 'posts_privilege_wiki.csv',\n",
    "             'posts_questions.csv', 'posts_tag_wiki.csv', 'stackoverflow_posts.csv',\n",
    "             'tags.csv', 'users.csv', 'votes.csv']\n",
    "json_folder = 'path/to/json_output'\n",
    "\n",
    "# Aggregate and convert each CSV file to JSON with NoSQL schema\n",
    "aggregate_and_convert_to_nosql(csv_files, json_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28c94d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "\n",
    "def aggregate_and_convert_to_nosql(csv_files, json_folder):\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(json_folder, exist_ok=True)\n",
    "\n",
    "    # Initialize a dictionary to store aggregated post data\n",
    "    aggregated_posts = {}\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        # Extract the table name from the CSV file name\n",
    "        table_name = os.path.splitext(os.path.basename(csv_file))[0]\n",
    "\n",
    "        # Open CSV file for reading\n",
    "        with open(csv_file, 'r', encoding='utf-8') as csv_file:\n",
    "            # Create a CSV reader\n",
    "            csv_reader = csv.DictReader(csv_file)\n",
    "\n",
    "            # Iterate through each row in the CSV file\n",
    "            for row_number, row in enumerate(csv_reader, start=1):\n",
    "                # Assuming 'id' is the unique identifier\n",
    "                post_id = row.get('id', '')\n",
    "\n",
    "                # Ignore rows with missing or incorrect values\n",
    "                if not post_id or not row.get('post_type_id'):\n",
    "                    continue\n",
    "\n",
    "                # Check if the post_id is not in the aggregated_posts dictionary\n",
    "                if post_id not in aggregated_posts:\n",
    "                    aggregated_posts[post_id] = {\n",
    "                        'post_id': post_id,\n",
    "                        'title': row.get('title', ''),\n",
    "                        'body': row.get('body', ''),\n",
    "                        'accepted_answer_id': row.get('accepted_answer_id', ''),\n",
    "                        'answer_count': int(row.get('answer_count', 0)) if row.get('answer_count', '').isdigit() else 0,\n",
    "                        'comment_count': int(row.get('comment_count', 0)),\n",
    "                        'community_owned_date': row.get('community_owned_date', ''),\n",
    "                        'creation_date': row.get('creation_date', ''),\n",
    "                        'favorite_count': int(row.get('favorite_count', 0)) if row.get('favorite_count', '').isdigit() else 0,\n",
    "                        'last_activity_date': row.get('last_activity_date', ''),\n",
    "                        'last_edit_date': row.get('last_edit_date', ''),\n",
    "                        'last_editor_display_name': row.get('last_editor_display_name', ''),\n",
    "                        'last_editor_user_id': row.get('last_editor_user_id', ''),\n",
    "                        'owner_display_name': row.get('owner_display_name', ''),\n",
    "                        'owner_user_id': row.get('owner_user_id', ''),\n",
    "                        'parent_id': row.get('parent_id', ''),\n",
    "                        'post_type_id': int(row.get('post_type_id', 0)),\n",
    "                        'score': int(row.get('score', 0)),\n",
    "                        'tags': row.get('tags', ''),\n",
    "                        'view_count': int(row.get('view_count', 0)) if row.get('view_count', '').isdigit() else 0,\n",
    "                        'answers': []\n",
    "                    }\n",
    "\n",
    "                # If the row is an answer, add it to the 'answers' list\n",
    "                if table_name == 'posts_answers':\n",
    "                    answer = {\n",
    "                        'answer_id': row.get('answer_id', ''),\n",
    "                        'answer_body': row.get('body', ''),  # Use the 'body' field for answer text\n",
    "                        'answer_user_id': row.get('owner_user_id', '')  # Use the 'owner_user_id' as the answer user_id\n",
    "                    }\n",
    "                    aggregated_posts[post_id]['answers'].append(answer)\n",
    "\n",
    "    # Construct the JSON file path for posts\n",
    "    json_file_path = os.path.join(json_folder, 'posts.json')\n",
    "\n",
    "    # Open JSON file for writing\n",
    "    with open(json_file_path, 'w', encoding='utf-8') as json_file:\n",
    "        # Write the aggregated post data to the JSON file\n",
    "        json.dump(list(aggregated_posts.values()), json_file, indent=2)\n",
    "\n",
    "# Specify the list of CSV files and the folder for JSON output files\n",
    "csv_files = ['badges.csv', 'post_history.csv', 'post_links.csv',\n",
    "             'posts_answers.csv', 'posts_moderator_nomination.csv',\n",
    "             'posts_orphaned_tag_wiki.csv', 'posts_privilege_wiki.csv',\n",
    "             'posts_questions.csv', 'posts_tag_wiki.csv', 'stackoverflow_posts.csv',\n",
    "             'tags.csv', 'users.csv', 'votes.csv']\n",
    "json_folder = 'path/to/json_output2'\n",
    "\n",
    "# Aggregate and convert each CSV file to JSON with NoSQL schema\n",
    "aggregate_and_convert_to_nosql(csv_files, json_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eece7a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
